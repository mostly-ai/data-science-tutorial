{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part IV: Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part of the tutorial covers model search and estimation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the cholesterol data from part I as a first example to clarify the basic concepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chol = pd.read_csv('cholesterol.csv')\n",
    "chol.Time = chol.Time.astype('category')\n",
    "chol.Margarine = chol.Margarine.astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is recoded so that the cholesterol measurements of each time point are available in a single row for each ID:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chol_spread = chol.set_index(['ID', 'Margarine', 'Time']).unstack()\n",
    "chol_spread.columns = ['T%d' % i for i in chol_spread.columns.levels[1]]\n",
    "chol_spread = chol_spread.reset_index(level=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Categorical features need to be one-hot-encoded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chol_spread = pd.get_dummies(chol_spread)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting dataset now has a column for each level of the categorical variables. `.head` can be used to inspect the structure of a DataFrame without polluting the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chol_spread.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The standard for machine learning in python is `sklearn`. We will use it here to split the dataset into train, validation and test subsets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "ids = chol_spread.index\n",
    "train_ids, test_ids = train_test_split(ids, train_size=0.75)\n",
    "train_ids, validation_ids = train_test_split(train_ids, test_size = 0.3)\n",
    "\n",
    "chol_train = chol_spread.loc[train_ids.values, :]\n",
    "chol_valid = chol_spread.loc[validation_ids.values, :]\n",
    "chol_test = chol_spread.loc[test_ids.values, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The three subsets are then always used in a very specific way.\n",
    "\n",
    " * The train dataset is used for fitting all models.\n",
    " * The validation dataset is used for finding the best model configuration (prediction variables, parameters, etc.).\n",
    " * The test dataset is only used to check the quality of the final model and should never be used to tune models.\n",
    "\n",
    "In this example pipeline, we will train two different linear models using different model specifications. Both models are trained on the training dataset. In practice, the total number of models trained can of course be much larger than two, perhaps using stepwise variable selection methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this first example, we will use linear regression to predict a numeric target variable, the cholesterol measurement at time point T3.\n",
    "\n",
    "We thus define `X` and `Y` as the predictor and target variables, respectively:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = chol_train.drop('T3', axis=1)\n",
    "y = chol_train['T3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define a linear regression model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1 = linear_model.LinearRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Then, we train the model on the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m1.fit(X, y)\n",
    "m1.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now, we can compute the mean squared error function on the validation set to assess the predictive quality of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_valid = chol_valid.drop('T3', axis=1)\n",
    "y_valid = chol_valid['T3']\n",
    "valid_pred = m1.predict(X_valid)\n",
    "metrics.mean_squared_error(y_pred=valid_pred, y_true=y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's try another more complex model that includes quadratic terms in addition to the linear terms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "m2 = Pipeline([('poly', PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)),\n",
    "                  ('linear', linear_model.LinearRegression(fit_intercept=True))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Again, we train a model on the training set..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m2.fit(X, y)\n",
    "m2.steps[1][1].coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and evaluate predictive quality on the validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_valid = chol_valid['T3']\n",
    "valid_pred = m2.predict(X_valid)\n",
    "metrics.mean_squared_error(y_pred=valid_pred, y_true=y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The validation error is much larger than for the smaller model, indicating that the complex model was overfitting. \n",
    "\n",
    "Based on the error estimations on the validation set, we choose to use the first model, and can now evaluate its error using the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test = chol_test.drop('T3', axis=1)\n",
    "y_test = chol_test['T3']\n",
    "\n",
    "test_pred = m1.predict(X_test)\n",
    "metrics.mean_squared_error(y_pred=test_pred, y_true=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a separate test set to compute the generalization error when the validation error was already computed on data not previously used by the estimator might seem odd at first in this case. However, especially when more than two models are begin compared and the best one selected based on it having the lowest validation error, then this validation error is a biased estimate of the real generalization error. The selected model might just be one of several equivalent models which just happens to have the lowest validation error between them purely due to random variation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now turn to classification. This example will be based on the 2017 Stack Overflow Developer Survey dataset already used in part III. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "so = pd.read_csv('survey_results_public.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this classification tast, we will look at a different subset of the dataset, to predict job satisfaction based on a number of other variables from the survey:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "so = pd.concat([\n",
    "        so.loc[:, 'Respondent'],\n",
    "        so.loc[:, 'HomeRemote':'YearsProgram'],\n",
    "        so.loc[:, 'ProblemSolving':'ChangeWorld'],\n",
    "        so.loc[:, ['ClickyKeys', 'Overpaid', 'TabsSpaces', 'JobSatisfaction', 'CareerSatisfaction', 'HaveWorkedLanguage']]\n",
    "     ], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Job satisfaction will be encoded as a binary variable, particioning the original answers into the categories `>= 7`, `< 7` and `NaN`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "so.loc[so.JobSatisfaction[so.JobSatisfaction.notnull()].index, 'JobSatisfaction'] = so.JobSatisfaction >= 7\n",
    "so.JobSatisfaction = so.JobSatisfaction.astype('category').cat.codes\n",
    "so.CareerSatisfaction = so.CareerSatisfaction.astype('category')\n",
    "so['UsesR'] = so.HaveWorkedLanguage.str.match('.*(R$)|.*(R;)') == True\n",
    "so.drop('HaveWorkedLanguage', axis=1, inplace=True)\n",
    "so.set_index('Respondent', inplace=True)\n",
    "\n",
    "ids = so.index.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we split the data into train, validation and test sets..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_ids, test_ids = train_test_split(ids, train_size=0.75)\n",
    "train_ids, validation_ids = train_test_split(train_ids, test_size = 0.3)\n",
    "\n",
    "so_train = so.loc[train_ids, :]\n",
    "so_valid = so.loc[validation_ids, :]\n",
    "so_test = so.loc[test_ids, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and differentiate between the target value `y` (`JobSatisfaction`), and the predictor variables `X`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = so_train.drop('JobSatisfaction', axis=1)\n",
    "y = so_train['JobSatisfaction']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The first model class to be used is logistic regression, which has the advantage of relatively easy interpretability due to it being based on the theoretical framework of the linear model. We define the model object first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1_logistic = linear_model.LogisticRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now we try to train the model on the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1_logistic.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This produces an error: scikit-learn cannot deal with categorical variables.\n",
    "See: http://scikit-learn.org/stable/modules/preprocessing.html#encoding-categorical-features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The solution is one-hot encoding, using pandas to get_dummies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = pd.get_dummies(so_train.drop('JobSatisfaction', axis=1), dummy_na=True)\n",
    "X_valid = pd.get_dummies(so_valid.drop('JobSatisfaction', axis=1), dummy_na=True)\n",
    "X_test = pd.get_dummies(so_test.drop('JobSatisfaction', axis=1), dummy_na=True)\n",
    "\n",
    "y = so_train['JobSatisfaction']\n",
    "y_valid = so_valid['JobSatisfaction']\n",
    "y_test = so_test['JobSatisfaction']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now apply the logistic regression model to the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m1_logistic = linear_model.LogisticRegression()\n",
    "m1_logistic.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We will try a different model using only a subset of features based on our a-priori hypothesis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cols = ['UsesR', 'TabsSpaces', 'ClickyKeys', 'Overpaid']\n",
    "X_small = pd.get_dummies(so_train.drop('JobSatisfaction', axis=1)[cols], dummy_na=True)\n",
    "X_valid_small = pd.get_dummies(so_valid.drop('JobSatisfaction', axis=1)[cols], dummy_na=True)\n",
    "X_test_small = pd.get_dummies(so_test.drop('JobSatisfaction', axis=1)[cols], dummy_na=True)\n",
    "\n",
    "m2_logistic = linear_model.LogisticRegression()\n",
    "m2_logistic.fit(X_small, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now compare the validation accuracy of model 1..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "valid_pred_m1 = m1_logistic.predict(X_valid)\n",
    "display(metrics.confusion_matrix(y_true=y_valid, y_pred=valid_pred_m1))\n",
    "metrics.accuracy_score(y_true=y_valid, y_pred=valid_pred_m1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and model 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "valid_pred_m2 = m2_logistic.predict(X_valid_small)\n",
    "display(metrics.confusion_matrix(y_true=y_valid, y_pred=valid_pred_m2))\n",
    "metrics.accuracy_score(y_true=y_valid, y_pred=valid_pred_m2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, it becomes clear that the simple model is missing some crucial information: the accuracy is much lower than that of the model with all variables, not only on the training set (which could be due to overfitting), but also on the validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random forests are a potentially more powerful class of models. Again, the implementation we use is from `sklearn`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "m_rf = RandomForestClassifier()\n",
    "m_rf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_pred_rf = m_rf.predict(X_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "To compare it with the other models, validation set accuracy of the random forest model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(metrics.confusion_matrix(y_true=y_valid, y_pred=valid_pred_rf))\n",
    "metrics.accuracy_score(y_true=y_valid, y_pred=valid_pred_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "First, we note that the accuracy is about the same as for glm...\n",
    "\n",
    "Maybe this can be improved by changing one of the parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_rf_2 = RandomForestClassifier(min_samples_leaf=10)\n",
    "m_rf_2.fit(X, y)\n",
    "\n",
    "valid_pred_rf_2 = m_rf_2.predict(X_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We can now compute the validation set accuracy of the second random forest model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "display(metrics.confusion_matrix(y_true=y_valid, y_pred=valid_pred_rf_2))\n",
    "metrics.accuracy_score(y_true=y_valid, y_pred=valid_pred_rf_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the validation accuracy has improved with the higher minimum number of samples per leaf (`min_samples_leaf`). Maybe we can try a third model with a still higher value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m_rf_3 = RandomForestClassifier(min_samples_leaf=20)\n",
    "m_rf_3.fit(X, y)\n",
    "\n",
    "valid_pred_rf_3 = m_rf_3.predict(X_valid)\n",
    "\n",
    "display(metrics.confusion_matrix(y_true=y_valid, y_pred=valid_pred_rf_3))\n",
    "metrics.accuracy_score(y_true=y_valid, y_pred=valid_pred_rf_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameter tuning is not a task to be performed manually, it can be automated for a more efficient search through the parameter space. A simple tuning method for a single parameter might look as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tune_rf(min_samples_leaf):\n",
    "    m_rf = RandomForestClassifier(min_samples_leaf=min_samples_leaf)\n",
    "    m_rf.fit(X, y)\n",
    "\n",
    "    valid_pred_rf = m_rf.predict(X_valid)\n",
    "\n",
    "    return metrics.accuracy_score(y_true=y_valid, y_pred=valid_pred_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "min_node_size_candidates = np.hstack([np.arange(1, 61, 10), np.arange(100, 1001, 300)])\n",
    "pd.DataFrame(np.vstack([min_node_size_candidates, np.array([tune_rf(m) for m in min_node_size_candidates])]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also try to tune multiple parameters, however, the number of models to be computed increases exponentially with the number of parameters to tune, so this method can get unwieldy rather quickly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tune_rf(min_samples_leaf, min_samples_split):\n",
    "    m_rf = RandomForestClassifier(min_samples_leaf=min_samples_leaf, min_samples_split=min_samples_split)\n",
    "    m_rf.fit(X, y)\n",
    "\n",
    "    valid_pred_rf = m_rf.predict(X_valid)\n",
    "\n",
    "    return metrics.accuracy_score(y_true=y_valid, y_pred=valid_pred_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[tune_rf(*c) for c in zip(np.repeat((20, 80, 800), 2), np.tile((2, 5), 3))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, more efficient heuristics are needed to find optimal parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Gradient Boosted Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The last method to be presented here, and one that is frequently found in winning entries at kaggle, is gradient \n",
    "boosting. We will use the package `xgboost` for this. (Windows users download from: https://www.lfd.uci.edu/~gohlke/pythonlibs/#xgboost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "m_xgb = xgb.XGBClassifier()\n",
    "m_xgb.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a model and compute the validation accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "valid_pred_xgb = m_xgb.predict(X_valid)\n",
    "\n",
    "display(metrics.confusion_matrix(y_true=y_valid, y_pred=valid_pred_xgb))\n",
    "metrics.accuracy_score(y_true=y_valid, y_pred=valid_pred_xgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Only after deciding for a final model, we can use the test dataset to evaluate the accuracy of final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_pred = m_xgb.predict(X_test)\n",
    "\n",
    "display(metrics.confusion_matrix(y_true=y_test, y_pred=test_pred))\n",
    "metrics.accuracy_score(y_true=y_test, y_pred=test_pred)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
